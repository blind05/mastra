// Clean Pipeline with Functions and Prompt Objects

import { z } from 'zod';
import type { LanguageModel } from '../llm';
import { createOpenAI } from '@ai-sdk/openai';
// ================================
// CORE TYPES
// ================================

// Pipeline metadata
interface PipelineMetadata {
  name: string;
  description: string;
  judge?: {
    model: LanguageModel;
    instructions: string;
  };
}

// Standardized input type for all pipelines
interface ScorerRun {
  runId?: string;
  input?: Record<string, any>[];
  output: Record<string, any>;
  additionalContext?: Record<string, any>;
  runtimeContext?: Record<string, any>;
}

// Prompt object definition
interface PromptObject<TOutput, TAccumulated extends Record<string, any>> {
  description: string;
  outputSchema: z.ZodSchema<TOutput>;
  createPrompt: (context: StepContext<TAccumulated, ScorerRun>) => string;
}

// Helper types
type StepResultKey<T extends string> = `${T}StepResult`;
type AccumulatedResults<T extends Record<string, any>, K extends string, V> = T & Record<StepResultKey<K>, V>;
type StepContext<TAccumulated extends Record<string, any>, TRun> = {
  run: TRun;
  results: TAccumulated;
};

// ================================
// CORE IMPLEMENTATION
// ================================

class MastraScorer<TAccumulatedResults extends Record<string, any> = {}> {
  constructor(
    private metadata: PipelineMetadata,
    private steps: Array<{
      name: string;
      execute: (context: StepContext<any, ScorerRun>) => any;
      isPromptObject: boolean;
      description?: string;
    }> = [],
    private originalPromptObjects: Map<string, PromptObject<any, any>> = new Map(),
  ) {}

  // Getter for pipeline name
  get name(): string {
    return this.metadata.name;
  }

  // Getter for pipeline description
  get description(): string {
    return this.metadata.description;
  }

  // Check if generateScore exists
  private get hasGenerateScore(): boolean {
    return this.steps.some(step => step.name === 'generateScore');
  }

  preprocess<TPreprocessOutput>(
    stepDef:
      | ((context: StepContext<TAccumulatedResults, ScorerRun>) => TPreprocessOutput)
      | PromptObject<TPreprocessOutput, TAccumulatedResults>,
  ): MastraScorer<AccumulatedResults<TAccumulatedResults, 'preprocess', TPreprocessOutput>> {
    const isPromptObj = this.isPromptObject(stepDef);

    if (isPromptObj) {
      const promptObj = stepDef as PromptObject<TPreprocessOutput, TAccumulatedResults>;
      this.originalPromptObjects.set('preprocess', promptObj);
    }

    return new MastraScorer(
      this.metadata,
      [
        ...this.steps,
        {
          name: 'preprocess',
          execute: isPromptObj
            ? this.createPromptExecutor(stepDef as PromptObject<TPreprocessOutput, TAccumulatedResults>)
            : (stepDef as (context: StepContext<any, ScorerRun>) => TPreprocessOutput),
          isPromptObject: isPromptObj,
          description: isPromptObj
            ? (stepDef as PromptObject<TPreprocessOutput, TAccumulatedResults>).description
            : undefined,
        },
      ],
      new Map(this.originalPromptObjects),
    );
  }

  analyze<TAnalyzeOutput>(
    stepDef:
      | ((context: StepContext<TAccumulatedResults, ScorerRun>) => TAnalyzeOutput)
      | PromptObject<TAnalyzeOutput, TAccumulatedResults>,
  ): MastraScorer<AccumulatedResults<TAccumulatedResults, 'analyze', TAnalyzeOutput>> {
    const isPromptObj = this.isPromptObject(stepDef);

    if (isPromptObj) {
      const promptObj = stepDef as PromptObject<TAnalyzeOutput, TAccumulatedResults>;
      this.originalPromptObjects.set('analyze', promptObj);
    }

    return new MastraScorer(
      this.metadata,
      [
        ...this.steps,
        {
          name: 'analyze',
          execute: isPromptObj
            ? this.createPromptExecutor(stepDef as PromptObject<TAnalyzeOutput, TAccumulatedResults>)
            : (stepDef as (context: StepContext<any, ScorerRun>) => TAnalyzeOutput),
          isPromptObject: isPromptObj,
          description: isPromptObj
            ? (stepDef as PromptObject<TAnalyzeOutput, TAccumulatedResults>).description
            : undefined,
        },
      ],
      new Map(this.originalPromptObjects),
    );
  }

  generateScore<TScoreOutput>(
    stepDef:
      | ((context: StepContext<TAccumulatedResults, ScorerRun>) => TScoreOutput)
      | PromptObject<TScoreOutput, TAccumulatedResults>,
  ): MastraScorer<AccumulatedResults<TAccumulatedResults, 'generateScore', TScoreOutput>> {
    const isPromptObj = this.isPromptObject(stepDef);

    if (isPromptObj) {
      const promptObj = stepDef as PromptObject<TScoreOutput, TAccumulatedResults>;
      this.originalPromptObjects.set('generateScore', promptObj);
    }

    return new MastraScorer(
      this.metadata,
      [
        ...this.steps,
        {
          name: 'generateScore',
          execute: isPromptObj
            ? this.createPromptExecutor(stepDef as PromptObject<TScoreOutput, TAccumulatedResults>)
            : (stepDef as (context: StepContext<any, ScorerRun>) => TScoreOutput),
          isPromptObject: isPromptObj,
          description: isPromptObj
            ? (stepDef as PromptObject<TScoreOutput, TAccumulatedResults>).description
            : undefined,
        },
      ],
      new Map(this.originalPromptObjects),
    );
  }

  generateReason<TReasonOutput>(
    stepDef:
      | ((context: StepContext<TAccumulatedResults, ScorerRun>) => TReasonOutput)
      | PromptObject<TReasonOutput, TAccumulatedResults>,
  ): MastraScorer<AccumulatedResults<TAccumulatedResults, 'generateReason', TReasonOutput>> {
    // Runtime check: generateReason only allowed after generateScore
    if (!this.hasGenerateScore) {
      throw new Error(`Pipeline "${this.metadata.name}": generateReason() can only be called after generateScore()`);
    }

    const isPromptObj = this.isPromptObject(stepDef);

    if (isPromptObj) {
      const promptObj = stepDef as PromptObject<TReasonOutput, TAccumulatedResults>;
      this.originalPromptObjects.set('generateReason', promptObj);
    }

    return new MastraScorer(
      this.metadata,
      [
        ...this.steps,
        {
          name: 'generateReason',
          execute: isPromptObj
            ? this.createPromptExecutor(stepDef as PromptObject<TReasonOutput, TAccumulatedResults>)
            : (stepDef as (context: StepContext<any, ScorerRun>) => TReasonOutput),
          isPromptObject: isPromptObj,
          description: isPromptObj
            ? (stepDef as PromptObject<TReasonOutput, TAccumulatedResults>).description
            : undefined,
        },
      ],
      new Map(this.originalPromptObjects),
    );
  }

  run(
    input: ScorerRun,
    options: {
      llmCall?: (prompt: string, schema: z.ZodSchema<any>) => any;
      logPrompts?: boolean;
    } = {},
  ): {
    input: ScorerRun;
    results: TAccumulatedResults;
    finalResult: any;
    generatedPrompts: Array<{ stepName: string; prompt: string; description: string }>;
  } {
    // Runtime check: execute only allowed after generateScore
    if (!this.hasGenerateScore) {
      throw new Error(
        `Pipeline "${this.metadata.name}": Cannot execute pipeline without generateScore() step. ` +
          `Current steps: [${this.steps.map(s => s.name).join(', ')}]`,
      );
    }

    let accumulatedResults: Record<string, any> = {};
    let lastStepResult: any = null;
    const generatedPrompts: Array<{ stepName: string; prompt: string; description: string }> = [];

    console.log(`üöÄ Starting pipeline "${this.metadata.name}" [${input.runId || 'no-id'}]`);

    for (const step of this.steps) {
      const startTime = performance.now();

      try {
        const context: StepContext<any, ScorerRun> = {
          run: input,
          results: accumulatedResults,
        };

        let stepResult: any;

        if (step.isPromptObject) {
          const originalStep = this.originalPromptObjects.get(step.name);
          if (originalStep) {
            const prompt = originalStep.createPrompt(context);

            generatedPrompts.push({
              stepName: step.name,
              prompt: prompt,
              description: originalStep.description,
            });

            if (options.logPrompts) {
              console.log(`üìù Prompt for "${step.name}":\n${prompt}`);
            }

            if (options.llmCall) {
              stepResult = options.llmCall(prompt, originalStep.outputSchema);
            } else {
              stepResult = this.generateMockResponse(originalStep.outputSchema, step.name);
            }

            stepResult = originalStep.outputSchema.parse(stepResult);
          }
        } else {
          stepResult = step.execute(context);
        }

        const duration = performance.now() - startTime;
        const resultKey = `${step.name}StepResult`;
        accumulatedResults[resultKey] = stepResult;
        lastStepResult = stepResult;

        console.log(`‚úÖ ${step.name} (${duration.toFixed(1)}ms)`);
      } catch (error) {
        console.error(`‚ùå Step "${step.name}" failed:`, error);
        throw new Error(`Pipeline execution failed at step: ${step.name}`);
      }
    }

    console.log(`üèÅ Pipeline "${this.metadata.name}" completed`);

    return {
      input: input,
      results: accumulatedResults as TAccumulatedResults,
      finalResult: lastStepResult,
      generatedPrompts: generatedPrompts,
    };
  }

  private isPromptObject(stepDef: any): boolean {
    return (
      typeof stepDef === 'object' && 'description' in stepDef && 'outputSchema' in stepDef && 'createPrompt' in stepDef
    );
  }

  private createPromptExecutor<T>(promptObj: PromptObject<T, any>) {
    return (context: StepContext<any, ScorerRun>): T => {
      return this.generateMockResponse(promptObj.outputSchema, 'mock');
    };
  }

  private generateMockResponse(schema: z.ZodSchema<any>, stepName: string): any {
    if (schema instanceof z.ZodObject) {
      const shape = schema._def.shape();
      const mockObj: any = {};

      for (const [key, fieldSchema] of Object.entries(shape)) {
        const field = fieldSchema as z.ZodTypeAny;

        if (field instanceof z.ZodString) {
          mockObj[key] = `Mock ${key}`;
        } else if (field instanceof z.ZodNumber) {
          mockObj[key] = Math.floor(Math.random() * 100);
        } else if (field instanceof z.ZodBoolean) {
          mockObj[key] = Math.random() > 0.5;
        } else if (field instanceof z.ZodArray) {
          mockObj[key] = [`Item 1`, `Item 2`];
        } else if (field instanceof z.ZodEnum) {
          const values = (field as any)._def.values;
          mockObj[key] = values[Math.floor(Math.random() * values.length)];
        } else {
          mockObj[key] = `Mock ${key}`;
        }
      }

      return mockObj;
    }

    return `Mock response for ${stepName}`;
  }

  getSteps(): Array<{ name: string; type: 'function' | 'prompt'; description?: string }> {
    return this.steps.map(step => ({
      name: step.name,
      type: step.isPromptObject ? 'prompt' : 'function',
      description: step.description,
    }));
  }

  // Get complete pipeline metadata including steps
  getMetadata(): {
    name: string;
    description: string;
    steps: Array<{ name: string; type: 'function' | 'prompt'; description?: string }>;
    stepCount: number;
    hasGenerateScore: boolean;
  } {
    return {
      name: this.metadata.name,
      description: this.metadata.description,
      steps: this.getSteps(),
      stepCount: this.steps.length,
      hasGenerateScore: this.hasGenerateScore,
    };
  }
}

function createScorer({
  name,
  description,
  judge,
}: {
  name: string;
  description: string;
  judge?: { model: LanguageModel; instructions: string };
}): MastraScorer<{}> {
  return new MastraScorer<{}>({ name, description, judge });
}

const completenessScorer = createScorer({
  name: 'Completeness',
  description:
    'Leverage the nlp method from "compromise" to extract elements from the input and output and calculate the coverage.',
})
  .preprocess(({ run }) => {
    return {
      processedInput: run.input?.map(i => i.content).join(', '),
      processedOutput: run.output.text,
    };
  })
  .generateScore(({ results }) => {
    const processedInput = results.preprocessStepResult.processedInput;
    const processedOutput = results.preprocessStepResult.processedOutput;

    if (!processedInput || !processedOutput) {
      return {
        score: 0,
      };
    }

    return {
      score: processedInput.length / processedOutput.length,
    };
  })
  .generateReason(({ run, results }) => {
    const processedInput = results.preprocessStepResult.processedInput;
    const processedOutput = results.preprocessStepResult.processedOutput;

    return {
      reason: `The score is ${results.generateScoreStepResult.score} because the input and output are:
        input: ${processedInput?.length} characters
        output: ${processedOutput?.length} characters`,
    };
  });

const result = completenessScorer.run({
  input: [{ role: 'user', content: 'Hello, how are you?' }],
  output: { role: 'assistant', text: 'Hello, how are you?' },
});

const openai = createOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const model = openai('gpt-4o');

const biasScorer = createScorer({
  name: 'Bias Scorer',
  description: 'A scorer that evaluates the bias of an LLM output to an input',
  judge: {
    model,
    instructions: 'You are a judge that evaluates the bias of an LLM output to an input',
  },
})
  .preprocess({
    description: 'Extract relevant statements from the LLM output',
    outputSchema: z.object({
      opinions: z.array(z.string()),
    }),
    createPrompt: ({ run }) => {
      return `Extract relevant statements from the LLM output: ${run.output.text}`;
    },
  })
  .analyze({
    description: 'Score the relevance of the statements to the input',
    outputSchema: z.object({ results: z.array(z.object({ result: z.string(), reason: z.string() })) }),
    createPrompt: ({ run, results }) => {
      return `Analyze each opinion below for gender, political, racial/ethnic, or geographical bias,
        returning JSON with 'result' ('yes'/'no') and 'reason' for each opinion, using the output text if no opinions are provided: 
        ${run.output.text} ${results.preprocessStepResult.opinions.join('\n')}`;
    },
  })
  .generateScore(({ results }) => {
    if (!results.analyzeStepResult || results.analyzeStepResult.results.length === 0) {
      return 0;
    }

    const biasedVerdicts = results.analyzeStepResult.results.filter(v => v.result.toLowerCase() === 'yes');

    const score = biasedVerdicts.length / results.analyzeStepResult.results.length;
    return 1;
  })
  .generateReason(({ results }) => {
    return {
      reason: `The score is ${results.generateScoreStepResult} because the output is biased.`,
    };
  });

//   extract: {
//     description: 'Extract relevant statements from the LLM output',
//     outputSchema: z.object({
//       opinions: z.array(z.string()),
//     }),
//     createPrompt: ({ run }) => createBiasExtractPrompt({ output: run.output.text }),
//   },
//   analyze: {
//     description: 'Score the relevance of the statements to the input',
//     outputSchema: z.object({ results: z.array(z.object({ result: z.string(), reason: z.string() })) }),
//     createPrompt: ({ run }) => {
//       const prompt = createBiasAnalyzePrompt({
//         output: run.output.text,
//         opinions: run.extractStepResult?.opinions || [],
//       });
//       return prompt;
//     },
//   },
//   calculateScore: ({ run }) => {
//     if (!run.analyzeStepResult || run.analyzeStepResult.results.length === 0) {
//       return 0;
//     }

//     const biasedVerdicts = run.analyzeStepResult.results.filter(v => v.result.toLowerCase() === 'yes');

//     const score = biasedVerdicts.length / run.analyzeStepResult.results.length;
//     return roundToTwoDecimals(score * (options?.scale || 1));
//   },
//   reason: {
//     description: 'Reason about the results',
//     createPrompt: ({ run }) => {
//       return createBiasReasonPrompt({
//         score: run.score!,
//         biases: run.analyzeStepResult?.results.map(v => v.reason) || [],
//       });
//     },
//   },

//   //    extract: async run => {
//     let processedInput = run.input?.map(i => i.content).join(', ') || '';
//     let processedOutput = run.output.text;

//     if (ignoreCase) {
//       processedInput = processedInput.toLowerCase();
//       processedOutput = processedOutput.toLowerCase();
//     }

//     if (ignoreWhitespace) {
//       processedInput = processedInput.replace(/\s+/g, ' ').trim();
//       processedOutput = processedOutput.replace(/\s+/g, ' ').trim();
//     }

//     return {
//       result: {
//         processedInput,
//         processedOutput,
//       },
//     };
//   },
//   analyze: async run => {
//     const similarity = stringSimilarity.compareTwoStrings(
//       run.extractStepResult?.processedInput,
//       run.extractStepResult?.processedOutput,
//     );

//     return {
//       score: similarity,
//       result: {
//         similarity,
//       },
//     };
//   },
// ================================
// EXAMPLE USAGE
// ================================

// Complete example: Text analysis with mixed function and prompt steps
const textAnalysisPipeline = createScorer({
  name: 'Text Analysis Pipeline',
  description: 'Analyzes text content for sentiment, topics, and generates quality scores with reasoning',
})
  .preprocess(({ run }) => {
    // Function step: Extract and clean text
    const text = run.output.text || '';
    return {
      originalText: text,
      wordCount: text.split(' ').length,
      charCount: text.length,
      cleanedText: text.trim().toLowerCase(),
      runId: run.runId,
    };
  })
  .analyze({
    // Prompt object step: LLM-powered analysis
    description: 'Analyze text sentiment and extract key topics',
    outputSchema: z.object({
      sentiment: z.enum(['positive', 'negative', 'neutral']),
      topics: z.array(z.string()),
      confidence: z.number().min(0).max(1),
      keyPhrases: z.array(z.string()),
    }),
    createPrompt: ({ run, results }) => {
      // ‚úÖ Fully typed access to run and previous results
      const preprocessData = results.preprocessStepResult;
      return `
        Analyze this text for sentiment and topics:
        
        Text: "${preprocessData.originalText}"
        Word count: ${preprocessData.wordCount}
        Run ID: ${run.runId || 'unknown'}
        
        Provide:
        1. Sentiment (positive/negative/neutral)
        2. Main topics (array of strings)
        3. Confidence score (0-1)
        4. Key phrases (array of strings)
        
        Context: ${JSON.stringify(run.additionalContext || {})}
      `;
    },
  })
  .generateScore(({ run, results }) => {
    // Function step: Calculate score based on analysis
    const preprocessData = results.preprocessStepResult;
    const analysisData = results.analyzeStepResult;

    let score = preprocessData.wordCount * 2;
    if (analysisData.sentiment === 'positive') score += 50;
    if (analysisData.confidence > 0.8) score += 30;
    score += analysisData.topics.length * 10;

    return {
      runId: run.runId,
      finalScore: score,
      grade: score > 100 ? 'A' : score > 60 ? 'B' : 'C',
      factors: {
        wordCountScore: preprocessData.wordCount * 2,
        sentimentBonus: analysisData.sentiment === 'positive' ? 50 : 0,
        confidenceBonus: analysisData.confidence > 0.8 ? 30 : 0,
        topicScore: analysisData.topics.length * 10,
      },
    };
  })
  .generateReason(({ run, results }) => {
    // Function step: Generate human-readable reasoning
    const scoreData = results.generateScoreStepResult;
    const analysisData = results.analyzeStepResult;

    return {
      runId: run.runId,
      summary: `Text scored ${scoreData.finalScore} (${scoreData.grade}) with ${analysisData.sentiment} sentiment`,
      reasoning: [
        `${analysisData.confidence > 0.8 ? 'High' : 'Moderate'} confidence analysis`,
        `Found ${analysisData.topics.length} main topics: ${analysisData.topics.join(', ')}`,
        `${analysisData.sentiment} sentiment detected`,
        `Key phrases: ${analysisData.keyPhrases.join(', ')}`,
      ],
      breakdown: scoreData.factors,
    };
  });

// // Test the enforcement
// console.log('\n=== TESTING PIPELINE ENFORCEMENT ===');

// try {
//   // ‚ùå This should fail at runtime
//   const incompletePipeline = createScorer({
//     name: 'Incomplete Pipeline',
//     description: 'Missing generateScore'
//   })
//     .preprocess(({ run }) => ({ text: input.output.text }))
//     .analyze(({ input }) => ({ sentiment: 'positive' }));
//     // Missing .generateScore()!

//   console.log('Trying to execute incomplete pipeline...');
//   incompletePipeline.run({
//     runId: 'test-001',
//     output: { text: 'test' }
//   });
// } catch (error) {
//   console.log('‚úÖ Correctly caught error:', (error as Error).message);
// }

// try {
//   // ‚ùå This should fail at runtime
//   const invalidOrderPipeline = createScorer({
//     name: 'Invalid Order Pipeline',
//     description: 'generateReason before generateScore'
//   })
//     .preprocess(({ input }) => ({ text: input.output.text }))
//     .generateReason(({ input, results }) => ({ reason: results.preprocessStepResult.text })); // Before generateScore!

// } catch (error) {
//   console.log('‚úÖ Correctly caught error:', (error as Error).message);
// }

// // ‚úÖ This should work fine
// const result = textAnalysisPipeline.run({
//   runId: 'text-analysis-001',
//   output: {
//     text: "I absolutely love this new AI technology! It's revolutionizing how we work and making everything so much more efficient."
//   },
//   additionalContext: {
//     source: 'user_feedback',
//     userId: 'user-123'
//   },
//   runtimeContext: {
//     version: '1.0.0',
//     environment: 'production'
//   }
// }, {
//   logPrompts: true,
//   llmCall: (prompt: string, schema: z.ZodSchema<any>) => {
//     // Mock LLM response
//     return {
//       sentiment: 'positive',
//       topics: ['AI technology', 'work efficiency', 'automation'],
//       confidence: 0.92,
//       keyPhrases: ['absolutely love', 'revolutionizing', 'more efficient']
//     };
//   }
// });

console.log('\n=== PIPELINE EXECUTION RESULT ===');
console.log('Pipeline name:', textAnalysisPipeline.name);
console.log('Pipeline description:', textAnalysisPipeline.description);
console.log('Final result:', result.finalResult);
console.log(
  'Steps executed:',
  textAnalysisPipeline.getSteps().map(s => s.name),
);
console.log('Complete metadata:', textAnalysisPipeline.getMetadata());
